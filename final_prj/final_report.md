### Final Project Report
# Training Convolutional Neural Network using different floating point formats
  * Tuan Nguyen
  * Deep Learning Spring 2017
  * Dr. Martin Hagan
  
## 1. Introduction

In recent years, Deep Learning or Deep Neural Network (DNN) has grown tremendously in its popularity and usefulness []. However, DNN is computationally intensive, power-hungry and very often limited by hardware capability. For many years, single-precision (float32) and double-precision (float64) floating point formats have been widely used as the default formats for DNN. 

However, using lower precision is growing rapidly as a trend in DNN research. Recent research [][][] shows that low-precision and very low precision are sufficient for training and running DNN. In addition, in 2016, NVIDIA introduced Pascal GPU architecture and CUDA 8 SDK that fully support half-precision (float16) floating point format and mixed-precision computing. The lower memory, higher speed, and less power consumption are main motivations behind this trending. 

This project is a small research to examine how different floating-point precisions affect on DNN's accuracy, speed and memory. Due to the limited time, a simplified version of Convolutional Neural Network based on LeNet5[] has been selected as the network to train. The selected datasets are MNIST[] and CIFAR-10[], which are well-known benmarking datasets for machine learning algorithms.

The rest of report is oraganized as following: Section 2 is a very short introduction to Floating Point Formats used in this research. Section 3 is a description about MNIST and CIFAR-10 datasets and Section 4 describes about Convolutional Neural Network design. Section 5 shortly describe about the hardware/software configuration in the experiment and results will be shown in Section 6. The final section is a short conclusion.
## 2. Floating Point Formats

## 3. Datasets

## 4. Convolutional Neural Network

## 5. Experimental Setup

## 6. Experimental Results

## 7. Conclusion

## Reference
